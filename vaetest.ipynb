{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a448b464-9f01-4d11-9288-69348ca40dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e2887b26094604875a016a96066c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ltxv-2b-0.9.8-distilled.safetensors:   0%|          | 0.00/6.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ltxv_model_path = hf_hub_download(\n",
    "    repo_id=\"Lightricks/LTX-Video\",\n",
    "    filename=\"./ltxv-2b-0.9.8-distilled.safetensors\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c89f6e-196c-41b5-8c24-08b079ee240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "from ltx_video.models.autoencoders.causal_video_autoencoder import CausalVideoAutoencoder\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "ltxv_model_path = './ltxv-2b-0.9.8-distilled.safetensors'\n",
    "vae = CausalVideoAutoencoder.from_pretrained(ltxv_model_path)\n",
    "vae.to(device)\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4141c6-708e-4eb0-9d19-17a740938e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info :  {'video_fps': 16.0}\n",
      "torch.Size([80, 128, 128, 3]) torch.uint8\n",
      "torch.Size([2, 3, 65, 128, 128]) tensor([224., 235., 222., 190., 146., 122.,  94.,  72.,  47.,  46.,  44.,  42.,\n",
      "         37.,  35.,  31.,  31.,  31.,  33.,  33.,  33.,  30.,  29.,  29.,  30.,\n",
      "         30.,  29.,  29.,  29.,  29.,  29.,  27.,  26.,  24.,  24.,  24.,  24.,\n",
      "         26.,  26.,  26.,  26.,  24.,  24.,  24.,  22.,  24.,  25.,  29.,  32.,\n",
      "         49., 113., 228., 221., 224., 218., 227., 230., 230., 230., 227., 227.,\n",
      "        227., 226., 226., 226., 226., 226., 226., 226., 226., 226., 225., 225.,\n",
      "        224., 224., 228., 228., 129.,  51.,  28.,  53.,  76.,  69.,  72.,  67.,\n",
      "         69.,  69.,  69.,  69.,  69.,  69.,  69.,  69.,  69.,  69.,  69.,  68.,\n",
      "         69.,  69.,  68.,  68.,  59.,  31.,  15.,  19.,  18.,  18.,  18.,  18.,\n",
      "         18.,  18.,  18.,  18.,  20.,  20.,  20.,  20.,  20.,  20.,  20.,  20.,\n",
      "         20.,  20.,  20.,  20.,  20.,  20.,  20.,  20.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/video.py:197: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "vid_path = '/workspace/sixteen128.mp4'\n",
    "\n",
    "video_frames, audio, info = read_video(vid_path)\n",
    "print(\"info : \", info)\n",
    "\n",
    "print(video_frames.shape, video_frames.dtype)  # (T, H, W, C)  # T: frame count, C=3\n",
    "video_frames = rearrange(video_frames.unsqueeze(dim=0).tile(2, 1, 1, 1, 1), 'b t h w c -> b c t h w').to(device).to(torch.float32)\n",
    "video_frames = video_frames[:, :, :65, :, :] # 8의 배수 + 1이어야 한다.\n",
    "print(video_frames.shape, video_frames[0][0][0][0])\n",
    "\n",
    "video_frames = (video_frames/255) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683cfa29-8f91-46f6-b6a6-f42e5ebde4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 9, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    aeoutput = vae.encode(video_frames)\n",
    "    latent = aeoutput.latent_dist.mode()\n",
    "    print(latent.shape)\n",
    "\n",
    "del aeoutput\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4c503f-b247-4187-8ffe-16cc9ab7c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 25, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "timestep = torch.ones(video_frames[:, :, :49, :, :].shape[0], device=device) * 0.1\n",
    "\n",
    "reconstructed_videos = vae.decode(\n",
    "    latent[:, :, :4, :, :], target_shape=video_frames[:, :, :49, :, :].shape, timestep=timestep\n",
    ").sample\n",
    "\n",
    "print(reconstructed_videos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34e9fdb-7514-4ae1-8f02-2b325a4f84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.io import read_video, write_video\n",
    "\n",
    "output_path = './reencoded_video12816test_2.mp4'\n",
    "recon_videos = (rearrange(reconstructed_videos, \"b c t h w -> b t h w c\")[0].cpu()/2 + 0.5) * 255\n",
    "\n",
    "write_video(\n",
    "    filename=output_path,\n",
    "    video_array=recon_videos,      # shape: (T, H, W, C)\n",
    "    fps=int(info['video_fps']),\n",
    "    video_codec='libx264',         # optional\n",
    "    options={\"crf\": \"18\"}          # optional: quality setting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b695c1-13c2-4c00-b1a4-c378c41baf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d95350f-b998-4895-bf54-f7a525d6ba1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([255, 255, 255], dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_videos[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d228f79b-8fc9-488b-8793-b9ec16b3b2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([217., 230., 239., 238., 237., 225., 217., 201., 166., 155., 144., 133.,\n",
       "        114., 100.,  81.,  67.,  56.,  51.,  45.,  41.,  40.,  39.,  38.,  36.,\n",
       "         37.,  37.,  36.,  33.,  28.,  27.,  30.,  33.,  33.,  33.,  33.,  33.,\n",
       "         33.,  34.,  33.,  33.,  33.,  31.,  30.,  30.,  30.,  30.,  29.,  29.,\n",
       "         29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  28.,  28.,  28.,  28.,\n",
       "         27.,  27.,  27.,  27.,  27.,  22.,  17.,  20.,  26.,  27.,  27.,  27.,\n",
       "         27.,  27.,  27.,  27.,  27.,  27.,  27.,  27.,  26.,  26.,  26.,  26.,\n",
       "         26.,  26.,  26.,  26.,  24.,  26.,  26.,  27.,  29.,  30.,  33.,  35.,\n",
       "         38.,  41.,  83., 163., 227., 227., 223., 223., 227., 222., 206., 214.,\n",
       "        227., 227., 228., 228., 226., 227., 227., 227., 227., 227., 227., 227.,\n",
       "        227., 227., 227., 227., 227., 227., 227., 227., 226., 226., 226., 226.,\n",
       "        226., 226., 226., 226., 226., 226., 226., 226., 226., 226., 225., 225.,\n",
       "        224., 224., 222., 221., 220., 220., 219., 219., 177.,  90.,  39.,  40.,\n",
       "         30.,  31.,  41.,  69.,  73.,  73.,  71.,  71.,  71.,  70.,  70.,  70.,\n",
       "         70.,  70.,  70.,  70.,  70.,  70.,  70.,  69.,  68.,  68.,  67.,  67.,\n",
       "         67.,  67.,  67.,  67.,  68.,  68.,  68.,  68.,  68.,  68.,  68.,  68.,\n",
       "         69.,  69.,  69.,  69.,  69.,  69.,  69.,  69.,  67.,  60.,  41.,  19.,\n",
       "         15.,  17.,  17.,  17.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,\n",
       "         19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,\n",
       "         19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,\n",
       "         19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,  19.,\n",
       "         19.,  19.,  19.,  19.], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_frames[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f36a61-a9c3-4705-bc1e-b958e3673f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# spatial 4×4를 1×1로 줄이기 위한 3D‑Conv\n",
    "#   kernel_size=(1,4,4), stride=(1,4,4)\n",
    "conv3d = nn.Conv3d(\n",
    "    in_channels=128,\n",
    "    out_channels=128,\n",
    "    kernel_size=(1, 4, 4),\n",
    "    stride=(1, 4, 4),\n",
    "    padding=0,\n",
    ").to(device)\n",
    "\n",
    "y = conv3d(latent).squeeze()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a719040-c9ae-470d-b550-b13b08072fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a57dec44-d714-4bc7-a1de-ce768591be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "class ConvNeXtV2Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        intermediate_dim: int,\n",
    "        dilation: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        padding = (dilation * (7 - 1)) // 2\n",
    "        self.dwconv = nn.Conv1d(\n",
    "            dim, dim, kernel_size=7, padding=padding, groups=dim, dilation=dilation\n",
    "        )  # depthwise conv\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, intermediate_dim)  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(intermediate_dim)\n",
    "        self.pwconv2 = nn.Linear(intermediate_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input x is sequence, tensor of (bs, seq_len, dim)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)  # b n d -> b d n\n",
    "        x = self.dwconv(x)\n",
    "        x = x.transpose(1, 2)  # b d n -> b n d\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return residual + x\n",
    "\n",
    "cnv = ConvNeXtV2Block(128, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d749a6a4-0fc7-4b6a-8f6d-3d4e6b6a131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 128])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.randn((2, 13, 32))\n",
    "out = cnv(rearrange(y, 'b d n -> b n d'))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa2cd1a9-c91c-4701-a67e-a504230faf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UpsampleHalveChannel(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose1d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_ch)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.up(x)                   # → [B, C/2, 2L]\n",
    "        x = x.transpose(1, 2)            # → [B, 2L, C/2]\n",
    "        x = self.norm(x)\n",
    "        return self.act(x)\n",
    "\n",
    "upconv = UpsampleHalveChannel(128, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "babe15b3-dddd-4c27-8eff-576e6738deb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upconv(rearrange(y, 'b d n -> b n d')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c2acf9e-4faa-4ad7-9b20-8ef1ec32b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Linear(128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cdc34dd-8c0e-4006-8895-fbaa6abf9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3d parameters: 262,272\n",
      "cnv parameters: 71,936\n",
      "layers parameters: 8,256\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in conv3d.parameters())\n",
    "print(f\"conv3d parameters: {total_params:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in cnv.parameters())\n",
    "print(f\"cnv parameters: {total_params*4:,}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in layers.parameters())\n",
    "print(f\"layers parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b453a4-0d4d-4d49-acfa-5bd36ccc1aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
